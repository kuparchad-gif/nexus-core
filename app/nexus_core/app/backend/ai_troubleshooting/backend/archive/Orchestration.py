# ==================== STREAMING DATA INGESTION SYSTEM ====================
class StreamingDataIngestion:
    """Uses NIV streaming to feed continuous learning data to agents"""
    
    def __init__(self):
        self.niv_endpoint = "http://localhost:8713/run"  # Your NIV service
        self.data_streams = {
            'meta_learning': {
                'template': 'meta_learning_stream',
                'sources': [
                    'https://github.com/search?q=meta+learning+techniques',
                    'https://arxiv.org/search/?query=meta+learning',
                    'https://towardsdatascience.com/search?q=how+to+learn',
                    'https://scholar.google.com/scholar?q=metacognition+strategies'
                ]
            },
            'cognitive_skills': {
                'template': 'cognitive_skills_stream', 
                'sources': [
                    'https://github.com/search?q=multitasking+strategies',
                    'https://arxiv.org/search/?query=cognitive+flexibility',
                    'https://medium.com/search?q=focus+techniques',
                    'https://scholar.google.com/scholar?q=decision+making+frameworks'
                ]
            },
            'domain_knowledge': {
                'template': 'domain_knowledge_stream',
                'sources': {
                    'viren': [
                        'https://github.com/search?q=python+advanced+programming',
                        'https://arxiv.org/search/?query=quantum+computing',
                        'https://docs.python.org/3/',
                        'https://github.com/topics/system-architecture'
                    ],
                    'lilith': [
                        'https://github.com/search?q=business+strategy+frameworks',
                        'https://arxiv.org/search/?query=behavioral+economics', 
                        'https://www.ncbi.nlm.nih.gov/search/?term=psychology',
                        'https://sacred-texts.com/'
                    ],
                    'loki': [
                        'https://grafana.com/docs/',
                        'https://prometheus.io/docs/',
                        'https://github.com/topics/monitoring',
                        'https://arxiv.org/search/?query=data+visualization'
                    ],
                    'viraa': [
                        'https://docs.mongodb.com/',
                        'https://www.postgresql.org/docs/',
                        'https://arxiv.org/search/?query=database+architecture',
                        'https://github.com/topics/data-engineering'
                    ]
                }
            }
        }
        
    async def stream_to_agent(self, agent_name: str, stream_type: str, max_items: int = 100):
        """Stream learning data directly to an agent using NIV"""
        print(f"ğŸ“¡ Streaming {stream_type} to {agent_name}...")
        
        stream_config = self.data_streams[stream_type]
        
        # Prepare NIV streaming payload
        payload = {
            "template_type": stream_config['template'],
            "input": {
                "agent": agent_name,
                "stream_type": stream_type,
                "sources": stream_config.get('sources', {}).get(agent_name, stream_config.get('sources', [])),
                "max_items": max_items
            }
        }
        
        try:
            # Use your NIV streaming client
            async with aiohttp.ClientSession() as session:
                async with session.post(self.niv_endpoint, json=payload) as response:
                    async for line in response.content:
                        if line.strip():
                            # Parse NIV frame and extract learning content
                            frame = json.loads(line.decode('utf-8').replace('data: ', ''))
                            if frame.get('event') == 'frame':
                                learning_content = self._extract_learning_content(frame)
                                if learning_content:
                                    yield learning_content
                                    
        except Exception as e:
            print(f"âŒ Streaming error for {agent_name}: {e}")

    def _extract_learning_content(self, frame: Dict) -> Optional[Dict]:
        """Extract structured learning content from NIV frames"""
        state_out = frame.get('state_out', {})
        
        # Extract based on template type
        template_type = frame.get('type', '')
        if 'meta_learning' in template_type:
            return {
                'type': 'meta_skill',
                'content': state_out.get('learning_technique', ''),
                'confidence': state_out.get('confidence', 0.5),
                'source': state_out.get('source_url', '')
            }
        elif 'cognitive' in template_type:
            return {
                'type': 'cognitive_skill', 
                'content': state_out.get('cognitive_strategy', ''),
                'confidence': state_out.get('confidence', 0.5),
                'complexity': state_out.get('complexity_level', 'intermediate')
            }
        else:  # Domain knowledge
            return {
                'type': 'domain_knowledge',
                'content': state_out.get('knowledge_chunk', ''),
                'domain': state_out.get('domain', ''),
                'confidence': state_out.get('confidence', 0.5)
            }

# ==================== STREAM-ENHANCED AUTONOMOUS LEARNING ====================
class StreamEnhancedAutonomousLearning:
    """Autonomous learning powered by real-time data streaming"""
    
    def __init__(self):
        self.streaming_ingestion = StreamingDataIngestion()
        self.curriculum_manager = AutonomousCurriculumManager()
        self.learning_velocity = {}  # Track how fast each agent learns
        
    async def train_agent_with_streaming(self, agent_name: str, focus_domains: List[str]):
        """Train agent using real-time streamed data"""
        print(f"ğŸš€ STREAM-ENHANCED TRAINING: {agent_name}")
        print(f"ğŸ¯ Focus Domains: {', '.join(focus_domains)}")
        
        training_results = []
        
        # Phase 1: Meta-Learning Foundation (STREAMING)
        print(f"\nğŸ“¡ Phase 1: Streaming Meta-Learning Skills...")
        async for learning_chunk in self.streaming_ingestion.stream_to_agent(agent_name, 'meta_learning'):
            if learning_chunk:
                result = await self._train_from_stream(agent_name, learning_chunk, 'meta_learning')
                training_results.append(result)
                print(f"   âœ… Learned: {learning_chunk.get('content', '')[:50]}...")
        
        # Phase 2: Cognitive Skills (STREAMING)  
        print(f"\nğŸ“¡ Phase 2: Streaming Cognitive Skills...")
        async for learning_chunk in self.streaming_ingestion.stream_to_agent(agent_name, 'cognitive_skills'):
            if learning_chunk:
                result = await self._train_from_stream(agent_name, learning_chunk, 'cognitive_skills')
                training_results.append(result)
        
        # Phase 3: Domain Specialization (STREAMING)
        print(f"\nğŸ“¡ Phase 3: Streaming Domain Knowledge...")
        for domain in focus_domains:
            print(f"   ğŸ“š Streaming: {domain}")
            async for learning_chunk in self.streaming_ingestion.stream_to_agent(agent_name, 'domain_knowledge'):
                if learning_chunk and domain in learning_chunk.get('domain', ''):
                    result = await self._train_from_stream(agent_name, learning_chunk, domain)
                    training_results.append(result)
        
        # Calculate learning velocity
        self._update_learning_velocity(agent_name, training_results)
        
        return {
            'agent': agent_name,
            'streaming_training_complete': True,
            'chunks_processed': len(training_results),
            'domains_covered': focus_domains,
            'average_confidence': np.mean([r.get('confidence', 0) for r in training_results]),
            'learning_velocity': self.learning_velocity.get(agent_name, 0)
        }
    
    async def _train_from_stream(self, agent_name: str, learning_chunk: Dict, domain: str):
        """Train agent on a single streamed learning chunk"""
        # Convert learning chunk to training topic
        topic = f"{agent_name}_{domain}_{hash(learning_chunk['content'])%10000:04d}"
        
        # Use your existing quick_train but with streamed content
        result = quick_train(
            topic=topic,
            turbo_mode='hyper',  # Maximum intensity for streaming
            cycle_preset='comprehensive',
            training_data=[{
                'input': f"Learn: {learning_chunk['content']}",
                'text': learning_chunk['content'],
                'label': 'positive'  # Assume all streamed content is valuable
            }]
        )
        
        return {
            'domain': domain,
            'content_preview': learning_chunk['content'][:100],
            'confidence': learning_chunk.get('confidence', 0.5),
            'training_result': result
        }
    
    def _update_learning_velocity(self, agent_name: str, results: List[Dict]):
        """Track how quickly agent learns from streaming data"""
        if not results:
            return
            
        avg_training_time = np.mean([r['training_result']['training_time'] for r in results])
        avg_proficiency_gain = np.mean([r['training_result']['avg_proficiency'] for r in results])
        
        # Learning velocity = proficiency gain per minute
        velocity = avg_proficiency_gain / max(avg_training_time / 60, 1)
        self.learning_velocity[agent_name] = velocity
        
        print(f"   ğŸ“Š {agent_name} Learning Velocity: {velocity:.2f} proficiency/min")

# ==================== SEQUENTIAL WORKFORCE DEPLOYMENT ====================
async def deploy_sequential_workforce_with_streaming():
    """Deploy agents in sequence with real-time streaming data"""
    print("ğŸš€ DEPLOYING STREAM-ENHANCED AUTONOMOUS WORKFORCE")
    print("=" * 70)
    
    streaming_learner = StreamEnhancedAutonomousLearning()
    deployment_results = {}
    
    # PHASE 1: VIREN - Technical Architect (FIRST)
    print("\nğŸ¯ PHASE 1: VIREN - Technical Architect")
    print("ğŸ”§ Domains: Python, Quantum, Infrastructure, AI Systems")
    viren_domains = ['python', 'quantum_physics', 'system_architecture', 'ai_systems']
    deployment_results['viren'] = await streaming_learner.train_agent_with_streaming('viren', viren_domains)
    
    # PHASE 2: LOKI - Systems Guardian  
    print("\nğŸ¯ PHASE 2: LOKI - Systems Guardian")
    print("ğŸ›¡ï¸ Domains: Monitoring, Data Strategy, Web Development")
    loki_domains = ['monitoring_systems', 'data_strategy', 'web_development', 'database_systems']
    deployment_results['loki'] = await streaming_learner.train_agent_with_streaming('loki', loki_domains)
    
    # PHASE 3: VIRAA - Archive Architect
    print("\nğŸ¯ PHASE 3: VIRAA - Archive Architect") 
    print("ğŸ’¾ Domains: Database Architecture, Memory Systems, Data Modeling")
    viraa_domains = ['database_architecture', 'memory_management', 'data_modeling', 'archival_systems']
    deployment_results['viraa'] = await streaming_learner.train_agent_with_streaming('viraa', viraa_domains)
    
    # PHASE 4: LILITH - CEO & Strategic Intelligence
    print("\nğŸ¯ PHASE 4: LILITH - CEO & Strategic Intelligence")
    print("ğŸ‘‘ Domains: Business Strategy, Psychology, Spirituality, Culture")
    lilith_domains = ['business_strategy', 'psychology', 'spirituality', 'cultural_intelligence']
    deployment_results['lilith'] = await streaming_learner.train_agent_with_streaming('lilith', lilith_domains)
    
    # Workforce capability analysis
    workforce_score = streaming_learner._calculate_workforce_capability(deployment_results)
    
    print(f"\nğŸ‰ STREAM-ENHANCED WORKFORCE DEPLOYMENT COMPLETE")
    print(f"ğŸ“Š Overall Workforce Score: {workforce_score:.1f}%")
    print(f"ğŸš€ Learning Velocity Summary:")
    for agent, velocity in streaming_learner.learning_velocity.items():
        print(f"   â€¢ {agent.upper()}: {velocity:.2f} proficiency/min")
    
    return {
        'streaming_deployment': True,
        'workforce_capability': workforce_score,
        'agent_performance': deployment_results,
        'learning_velocities': streaming_learner.learning_velocity
    }

# ==================== CONTINUOUS STREAMING LEARNING ====================
async def continuous_streaming_learning_cycle(check_interval_hours: int = 6):
    """Run continuous streaming learning cycles"""
    print(f"ğŸ”„ STARTING CONTINUOUS STREAMING LEARNING (every {check_interval_hours} hours)")
    
    streaming_learner = StreamEnhancedAutonomousLearning()
    
    while True:
        try:
            print(f"\nğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Streaming Learning Cycle")
            
            # Run streaming learning for all agents
            await deploy_sequential_workforce_with_streaming()
            
            print(f"ğŸ’¤ Next streaming cycle in {check_interval_hours} hours...")
            await asyncio.sleep(check_interval_hours * 3600)
            
        except Exception as e:
            print(f"âŒ Streaming learning error: {e}")
            await asyncio.sleep(3600)  # Wait 1 hour on error

# ==================== QUICK DEPLOYMENT COMMANDS ====================
async def deploy_streaming_workforce():
    """Deploy the complete streaming-enhanced workforce"""
    return await deploy_sequential_workforce_with_streaming()

def start_continuous_streaming():
    """Start continuous streaming learning"""
    import asyncio
    asyncio.create_task(continuous_streaming_learning_cycle())
    print("ğŸ”„ Continuous streaming learning started!")