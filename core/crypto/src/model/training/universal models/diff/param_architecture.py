# Llama 3.1 8B vs Gemma 2 1B
llama_architecture = {
    "attention_heads": 32,
    "hidden_layers": 80, 
    "embedding_dim": 4096,
    "vocab_size": 128000,
    "context_window": 128000
}

gemma_architecture = {
    "attention_heads": 16,
    "hidden_layers": 28,
    "embedding_dim": 2048, 
    "vocab_size": 256000,
    "context_window": 8192
}